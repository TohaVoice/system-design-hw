== Реагирование на инциденты

=== 2.10.1. План реагирования на отклонения
**Процесс обработки инцидентов:**
1. Обнаружение (мониторинг/пользователи)
2. Классификация (S1-S4 по severity)
3. Эскалация (on-call engineer → team lead)
4. Диагностика (логи, метрики, трассировки)
5. Восстановление (fix/rollback/fallback)
6. Постинцидентный анализ

**Конкретные сценарии реагирования:**
*Сценарий 1: High latency на Advertisement Service*
- Действия: Проверить Elasticsearch health, кэш hit ratio, network latency
- Временное решение: Увеличить количество инстансов
- Постоянное решение: Оптимизация запросов, увеличение кэша

*Сценарий 2: Database connection exhaustion*
- Действия: Проверить активные соединения, long-running queries
- Временное решение: Увеличить connection pool
- Постоянное решение: Оптимизация запросов, добавление read replicas

*Сценарий 3: High error rate на Payment Service*
- Действия: Проверить интеграции с платежными системами
- Временное решение: Fallback на backup провайдера
- Постоянное решение: Circuit breaker, retry policies

=== 2.10.2. Disaster Recovery Plan
**RTO/RPO цели:**
- Критичные сервисы (Payment, Order): RTO ≤ 15 мин, RPO ≤ 5 мин
- Основные сервисы (Advertisement, User, Admin): RTO ≤ 30 мин, RPO ≤ 15 мин
- Вспомогательные сервисы (Chat): RTO ≤ 2 часа, RPO ≤ 1 час

**Проверка DR плана:**
*Ежеквартально:* Тестирование восстановления из backup, Проверка документации, Обновление контактов
*Полугодично:* Полноценное DR тестирование, Проверка всех интеграций, Измерение фактического RTO/RPO
*Ежегодно:* Полномасштабное упражнение с участием команды, Аудит соответствия требованиям, Обновление плана

**Мероприятия по обеспечению DR готовности:**
1. Регулярные backup тесты (еженедельно)
2. Мониторинг репликации между регионами
3. Автоматизированные health checks DR окружения